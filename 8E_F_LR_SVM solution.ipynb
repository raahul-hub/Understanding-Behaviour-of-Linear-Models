{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HExLQrE4ZxR"
   },
   "source": [
    "<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LuKrFzC4ZxV"
   },
   "source": [
    "<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wES-wWN4ZxX"
   },
   "source": [
    "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
    "\n",
    "Check the documentation for better understanding of these attributes: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
    "\n",
    "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
    "\n",
    "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
    "\n",
    "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
    "\n",
    "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
    "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
    "\n",
    "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
    "\n",
    "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z830CfMk4Zxa"
   },
   "source": [
    "## Task E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuBxHiCQ4Zxc"
   },
   "source": [
    "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
    "\n",
    "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
    "\n",
    "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fCgMNEvI4Zxf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ANUNIqCe4Zxn"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHie1zqH4Zxt"
   },
   "source": [
    "### Pseudo code\n",
    "\n",
    "clf = SVC(gamma=0.001, C=100.)<br>\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
    "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
    "    \n",
    "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
    "\n",
    "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try for Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linear_clf = LinearSVC()\n",
    "linear_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = linear_clf.intercept_[0]\n",
    "decision_func_op = []\n",
    "for i in range(len(X_cv)):\n",
    "    prod = np.dot(X_cv[i], linear_clf.coef_[0]) + intercept\n",
    "    decision_func_op.append(prod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.12725643, -1.08019728, -1.07912335,  0.00963751, -0.4754098 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(decision_func_op)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.12725643, -1.08019728, -1.07912335,  0.00963751, -0.4754098 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_clf.decision_function(X_cv)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the values of the implemented function matches with the model output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR RBF KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For RBF Kernel\n",
    "from numpy import linalg  \n",
    "\n",
    "clf = SVC(gamma=0.001, C=100, kernel='rbf')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Support Vectors\n",
    "Xi = clf.support_vectors_\n",
    "#Xi = clf.support_vectors_.tolist()\n",
    "\n",
    "#Query Point\n",
    "Xq = X_cv\n",
    "#Xq = X_cv.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_func(data, support_vectors, clf, gamma = 0.001 ):\n",
    "    \n",
    "    aiyi = clf.dual_coef_.tolist()\n",
    "    intercept = clf.intercept_[0]\n",
    "    decision_function_outputs = []\n",
    "    \n",
    "    #Selecting Query points\n",
    "    for query_point in data:\n",
    "        \n",
    "        interim_sum = 0\n",
    "        \n",
    "        #Selecting Each Support Vector\n",
    "        for i in range(len(support_vectors)):\n",
    "            dist = query_point - support_vectors[i]\n",
    "            l2_norm = linalg.norm(dist,2)\n",
    "            interim_val = math.exp(-gamma * math.pow(l2_norm,2))\n",
    "            interim_sum += aiyi[0][i] * interim_val\n",
    "        final_val = np.round((interim_sum + intercept),8)\n",
    "        decision_function_outputs.append(final_val)\n",
    "        \n",
    "    return np.array(decision_function_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.06763167, -2.22715144, -2.03757667, -0.03701432, -0.97272742])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_function_output = decision_func(X_cv, Xi, clf)\n",
    "decision_function_output[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.06763167, -2.22715144, -2.03757667, -0.03701432, -0.97272742])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.decision_function(X_cv)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0bKCboN4Zxu"
   },
   "source": [
    "<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMn7OEN94Zxw"
   },
   "source": [
    "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
    "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0n5EFkx4Zxz"
   },
   "source": [
    "## TASK F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0HOqVJq4Zx1"
   },
   "source": [
    "\n",
    "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
    "\n",
    "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
    "\n",
    "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
    "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
    "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
    "\n",
    "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTY7z2bd4Zx2"
   },
   "source": [
    "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM3odN1Z4Zx3"
   },
   "source": [
    "\n",
    "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
    "\n",
    "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
    "\n",
    "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
    "\n",
    "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
    "\n",
    "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here first task is to find y+ and y-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(693, 307)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = y_cv.tolist()\n",
    "targets.count(0), targets.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " #calculate y+, y-\n",
    "N_plus = targets.count(1)\n",
    "y_pos = (N_plus + 1) / (N_plus + 2)\n",
    "\n",
    "N_minus = targets.count(0)\n",
    "y_neg = 1 / (N_minus + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9967637540453075, 0.0014388489208633094)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pos, y_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00143885, 0.00143885, 0.00143885, 0.99676375, 0.00143885])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_modified = [y_pos if i == 1 else y_neg for i in y_cv]\n",
    "y_modified = np.array(y_modified)\n",
    "y_modified[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First need to modify **Sigmoid and Loss Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here use manual implementation of SGD code from previous assignmemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y_modified,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "    sub_sum = 0\n",
    "\n",
    "    for i in range(len(y_modified)):\n",
    "        sub_x = y_modified[i]*(np.log10(y_pred[i])) + (1 - y_modified[i])*(np.log10(1-y_pred[i]))\n",
    "        sub_sum += sub_x\n",
    "    loss = -(sub_sum)/len(y_true)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50\n",
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "tolerace = 0.00001\n",
    "\n",
    "#from tqdm import tqdm\n",
    "\n",
    "def initialize_weights(dim):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    #initialize the weights to zeros array of (1,dim) dimensions\n",
    "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
    "    #initialize bias to zero\n",
    "    \n",
    "    w = np.zeros_like(dim)\n",
    "    b = 0\n",
    "\n",
    "    return w,b\n",
    "\n",
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    sigmoid = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    return sigmoid\n",
    "\n",
    "def logloss(y_true,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "    sub_sum = 0\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        sub_x = y_true[i]*(np.log10(y_pred[i])) + (1 - y_true[i])*(np.log10(1-y_pred[i]))\n",
    "        sub_sum += sub_x\n",
    "    loss = -(sub_sum)/len(y_true)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    \n",
    "    dw = x*(y - sigmoid(np.dot(w, x+b))) - (alpha*w)/N\n",
    "    \n",
    "\n",
    "    return dw\n",
    "\n",
    "def gradient_db(x,y,w,b):\n",
    "    \n",
    "    '''In this function, we will compute gradient w.r.to b '''\n",
    "\n",
    "    db = y - sigmoid(np.dot(w, x+b))\n",
    "\n",
    "    return db\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train ,X_test, y_test, epochs ,alpha, eta0, tolerace):\n",
    "    \n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "\n",
    "    N=len(X_train)\n",
    "\n",
    "    w, b = initialize_weights(X_train[0])\n",
    "    training_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    #converged = False\n",
    "    \n",
    "    '''while not converged :\n",
    "        for every_epoch in tqdm(range(epochs)):'''\n",
    "\n",
    "    y_predicted_train = []\n",
    "    y_predicted_test = []\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        grad_w = gradient_dw(X_train[i], y_train[i], w, b, alpha, N)\n",
    "        grad_B = gradient_db(X_train[i], y_train[i], w, b)\n",
    "\n",
    "        #Updating weights and intercept\n",
    "        w = w + eta0 * grad_w\n",
    "        b = b + eta0 * grad_B\n",
    "\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        y_pred_train = sigmoid(np.dot(w,X_train[i])+b) #changed here, using sigmoid\n",
    "        y_predicted_train.append(y_pred_train)\n",
    "\n",
    "    loss_train = logloss(list(y_train), y_predicted_train)\n",
    "    training_loss.append(loss_train)\n",
    "\n",
    "    #predicting y_test with updated values of w and b\n",
    "    for i in range(len(X_test)):\n",
    "        y_pred_test = sigmoid(np.dot(w,X_test[i])+b)\n",
    "        y_predicted_test.append(y_pred_test)\n",
    "\n",
    "    loss_test = logloss(y_test, y_predicted_test)\n",
    "    test_loss.append(loss_test)\n",
    "\n",
    "    ''''if every_epoch != 0 and (training_loss[every_epoch-1] - training_loss[every_epoch]) <= tolerace:\n",
    "                max_epoch = every_epoch\n",
    "                converged = True\n",
    "                print(\"Converged at {0} th epoch with tolerance = {1} \".format(every_epoch, tolerace))\n",
    "                break'''\n",
    " \n",
    "    return w,b #,training_loss,test_loss, y_predicted_train\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.015193200827233874, -0.019355390099394014)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = train(y_cv, y_modified ,X_test, y_test, epochs ,alpha, eta0, tolerace)\n",
    "w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have value of w and b, write a function to compute probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def decision_function_linearSVM(data, classifier):\n",
    "    \n",
    "    decision_func_op = []\n",
    "    for i in range(len(data)):\n",
    "        prod = np.dot(data[i], classifier.coef_[0]) + classifier.intercept_[0]\n",
    "        decision_func_op.append(prod)\n",
    "    \n",
    "    return decision_func_op\n",
    "\n",
    "ftest = decision_function_linearSVM(X_test, classifier=linear_clf)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftest = decision_func(X_cv, Xi, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "platts_prob = []\n",
    "for i in range(len(X_test)):\n",
    "    prob = np.round(1/(1+math.exp(w*ftest[i]+b)),5)\n",
    "    platts_prob.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.51269,\n",
       " 0.5133,\n",
       " 0.51258,\n",
       " 0.50498,\n",
       " 0.50853,\n",
       " 0.50762,\n",
       " 0.5149,\n",
       " 0.51017,\n",
       " 0.49946,\n",
       " 0.49805]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "platts_prob[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
